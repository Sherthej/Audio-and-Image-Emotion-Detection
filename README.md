ğŸ­ Multimodal Emotion Recognition System
A hybrid emotion recognition system that analyzes facial expressions and vocal tone to identify human emotions from uploaded images and audio/video files. Built with Python, Whisper, Keras, Librosa, OpenCV, and hosted in Google Colab.
ğŸš€ Features
â€¢	â€¢ Speech Recognition & Transcription using OpenAI Whisper
â€¢	â€¢ Audio Emotion Detection via MFCC + Pitch features and SVM
â€¢	â€¢ Facial Emotion Detection using CNN on grayscale face images
â€¢	â€¢ Language Detection from spoken audio
â€¢	â€¢ Modular design for future integration (e.g., text-based emotion, video stream)
ğŸ› ï¸ Technologies Used
Component	Technology
Speech-to-Text	Whisper (base model)
Facial Emotion	Keras CNN
Audio Emotion	Librosa + Scikit-learn SVM
Visualization	OpenCV + cv2_imshow (Colab)
Audio Extraction	MoviePy
Interface	Google Colab
ğŸ“ Project Structure

emotion_recognition/
â”œâ”€â”€ emotion_model.h5             # Trained CNN model for facial emotion
â”œâ”€â”€ main_colab_notebook.ipynb    # Main Colab notebook (you run this)
â”œâ”€â”€ sample_inputs/               # Example .jpg and .mp4 files (optional)
â”œâ”€â”€ README.md                    # Project documentation

ğŸ“¦ Installation & Setup
â€¢	â€¢ Open in Google Colab
â€¢	â€¢ Upload Files (Image: .jpg/.png, Audio/Video: .mp3/.wav/.mp4)
â€¢	â€¢ Get Results: Facial Emotion (OpenCV) + Audio Emotion + Language + Transcript
ğŸ“Š Emotion Labels
Facial: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral
Audio: Happy, Sad, Angry, Neutral
(Text-based emotion classification can be added later)
âš™ï¸ How It Works (Pipeline)
â€¢	â€¢ Upload an image and/or audio/video file.
â€¢	â€¢ Image is passed through OpenCV â†’ Haar cascade â†’ CNN for emotion prediction.
â€¢	â€¢ Audio is converted to WAV â†’ transcribed via Whisper â†’ MFCC+Pitch features extracted.
â€¢	â€¢ Audio features are classified using an SVM into emotion labels.
â€¢	â€¢ Results are printed and/or shown with annotated images.
ğŸ“ˆ Future Improvements
â€¢	â€¢ Fusion-based multimodal deep model
â€¢	â€¢ Real emotion-labeled audio training datasets
â€¢	â€¢ Live webcam/video stream input
â€¢	â€¢ Deployable mobile or edge version using TensorFlow Lite
â€¢	â€¢ Text emotion classification from Whisper transcripts
â— Limitations
â€¢	â€¢ Audio classifier currently trained on random synthetic data
â€¢	â€¢ No true integration/fusion between facial and audio predictions
â€¢	â€¢ Not robust to poor lighting, occlusions, or high-noise audio
ğŸ“„ License
This project is intended for educational and research purposes only. If you use it, please give credit to the developers and mention this repo.
ğŸ‘¨â€ğŸ’» Author
Developed by [Your Name]
Email: your.email@example.com
Institution: [Your College or Organization]
ğŸ”— References
â€¢	â€¢ OpenAI Whisper
â€¢	â€¢ FER2013 Facial Emotion Dataset
â€¢	â€¢ Librosa Audio Feature Extraction
â€¢	â€¢ Scikit-learn
â€¢	â€¢ Keras & TensorFlow
â€¢	â€¢ MoviePy for audio extraction
